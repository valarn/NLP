{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: Vala Rahmani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "## **********************************Vegan Paleo Data ************************************##\n",
    "paleo = pd.read_csv('dataset/paleo_posts.csv')\n",
    "vegan = pd.read_csv('dataset/vegan_posts.csv')\n",
    "data  = pd.concat([paleo, vegan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4848, 104)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all the duplicates based on title that are in our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the duplicate titles from the dataset.\n",
    "data.drop_duplicates(subset = ['title'], keep = 'last',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 104)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['selftext'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vegan    1145\n",
       "Paleo     638\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['selftext'] = data['selftext'].fillna(value = 'notext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are usable in the \n",
    "usable_columns = ['id','author', 'is_video','name','num_comments',\n",
    "                  'score','selftext','subreddit','title','ups']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "author             0\n",
       "is_video           0\n",
       "name               0\n",
       "num_comments       0\n",
       "score              0\n",
       "selftext        1012\n",
       "subreddit          0\n",
       "title              0\n",
       "ups                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[usable_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[usable_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the interested data except the target column\n",
    "features = [i for i in usable_columns if i != 'subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>is_video</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>a0k6zx</td>\n",
       "      <td>arav24</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_a0k6zx</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi guys! I am majoring in Nutritional Science ...</td>\n",
       "      <td>I'm a 18 year old aspiring blogger [blogspam]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>cb2t2d</td>\n",
       "      <td>techguySF</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_cb2t2d</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did Bone Marrow make us Human? [Discussion]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     author  is_video       name  num_comments  score  \\\n",
       "673  a0k6zx     arav24     False  t3_a0k6zx             0      0   \n",
       "684  cb2t2d  techguySF     False  t3_cb2t2d             1      0   \n",
       "\n",
       "                                              selftext  \\\n",
       "673  Hi guys! I am majoring in Nutritional Science ...   \n",
       "684                                                NaN   \n",
       "\n",
       "                                             title  ups  \n",
       "673  I'm a 18 year old aspiring blogger [blogspam]    0  \n",
       "684    Did Bone Marrow make us Human? [Discussion]    0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1783.000000</td>\n",
       "      <td>1783.000000</td>\n",
       "      <td>1783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.611890</td>\n",
       "      <td>94.482894</td>\n",
       "      <td>94.482894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31.998925</td>\n",
       "      <td>348.249139</td>\n",
       "      <td>348.249139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>655.000000</td>\n",
       "      <td>4937.000000</td>\n",
       "      <td>4937.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_comments        score          ups\n",
       "count   1783.000000  1783.000000  1783.000000\n",
       "mean      13.611890    94.482894    94.482894\n",
       "std       31.998925   348.249139   348.249139\n",
       "min        0.000000     0.000000     0.000000\n",
       "25%        2.000000     3.000000     3.000000\n",
       "50%        7.000000    13.000000    13.000000\n",
       "75%       15.000000    57.000000    57.000000\n",
       "max      655.000000  4937.000000  4937.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the data\n",
    "* For the number of the comments I will be using the median as the threshold therefore if the number of the comments is greater than 6 num_comments will be one and if less than 6 will be 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['num_comments'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the posts that get more than the median comment a value of 1 is given and a value of 0 for less comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " data['num_comments'] = (data['num_comments']> data['num_comments'].median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.get_dummies(data, columns=['subreddit'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64% of the data is from the Vegan subreddit and the 36% from the Paleo subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.642176\n",
       "0    0.357824\n",
       "Name: subreddit_vegan, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subreddit_vegan'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data to better understand the content of each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'is_video', 'name', 'num_comments', 'score', 'selftext',\n",
       "       'title', 'ups', 'subreddit_vegan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['selftext'] = data['selftext'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are a lot of rows missing selftext, selftext is added to the title to create a row with most content possible.\n",
    "data['title_selftext'] = data['title'] + \" \" + data['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title_selftext'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # 1. Remove the html from the text to make the model more robust\n",
    "    nohtml = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    # 2. Remove the non-letter characters from the text\n",
    "    text_letters = re.sub(\"[^a-zA-Z]\", \" \", nohtml)\n",
    "    \n",
    "    # 3. Making everything lower case \n",
    "    lower_case = text_letters.lower()\n",
    "    \n",
    "    # 4. tokenizing based on the spaces\n",
    "    tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "    words = tokenizer.tokenize(lower_case)\n",
    "    # 5. Removing the stop words\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Adding some of the words that may leak data into our model\n",
    "    stopwords_list.extend(['paleo','vegan', 'needing help','needing','vegans','question','discussion','food pic','pic','food','blogspam'])\n",
    "    \n",
    "    stops = set(stopwords_list)\n",
    "    words_nostop = [w for w in words if w not in stops]\n",
    "    \n",
    "    \n",
    "    return (\" \".join(words_nostop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the title columns for each post\n",
      "There are total of 1783 titles\n",
      "Title Cleaning is complete!\n",
      "Title_Selftext Cleaning is also complete!\n",
      "--- Took 1.5788700580596924 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print('Cleaning the title columns for each post')\n",
    "print(f\"There are total of {data.shape[0]} titles\")\n",
    "counter= 0 \n",
    "data['clean_title'] = data['title'].apply(text_cleaner)\n",
    "print(\"Title Cleaning is complete!\")\n",
    "data['clean_title_selftext'] = data['title_selftext'].apply(text_cleaner)\n",
    "print(\"Title_Selftext Cleaning is also complete!\")\n",
    "print(\"--- Took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for text in data['title']:\n",
    "#     clean_titles.app(text_cleaner(text))\n",
    "#     data['clean_title1'][counter]= text_cleaner(text)\n",
    "#     if (counter+1) % 100 == 0:\n",
    "#         print(f\"{counter+1} of {data.shape[0]} titles have been cleaned up\")\n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cleaned up data to use in the modeling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data['clean_title'].isna().sum())\n",
    "print(data['clean_title_selftext'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>is_video</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_vegan</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_title_selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>a0k6zx</td>\n",
       "      <td>arav24</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_a0k6zx</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi guys! I am majoring in Nutritional Science ...</td>\n",
       "      <td>I'm a 18 year old aspiring blogger [blogspam]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm a 18 year old aspiring blogger [blogspam] ...</td>\n",
       "      <td>year old aspiring blogger</td>\n",
       "      <td>year old aspiring blogger hi guys majoring nut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  author  is_video       name  num_comments  score  \\\n",
       "673  a0k6zx  arav24     False  t3_a0k6zx             0      0   \n",
       "\n",
       "                                              selftext  \\\n",
       "673  Hi guys! I am majoring in Nutritional Science ...   \n",
       "\n",
       "                                             title  ups  subreddit_vegan  \\\n",
       "673  I'm a 18 year old aspiring blogger [blogspam]    0                0   \n",
       "\n",
       "                                        title_selftext  \\\n",
       "673  I'm a 18 year old aspiring blogger [blogspam] ...   \n",
       "\n",
       "                   clean_title  \\\n",
       "673  year old aspiring blogger   \n",
       "\n",
       "                                  clean_title_selftext  \n",
       "673  year old aspiring blogger hi guys majoring nut...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dataset/clean_data_vegan.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic modeling to get a feel for the data\n",
    "\n",
    "*Let's add Tfidf to our model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saved = pd.read_csv('dataset/clean_data_vegan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_title_selftext'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************** Vegan Paleo Train Test Split *******************************#\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = [i for i in data.columns if i!='subreddit_vegan']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                data['subreddit_vegan'],\n",
    "                                                random_state=42,\n",
    "                                                stratify=data['subreddit_vegan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer(ngram_range=(1,2),\n",
    "                       analyzer = \"word\",\n",
    "                       max_features=2500,\n",
    "                       tokenizer=None,\n",
    "                       stop_words=None)\n",
    "\n",
    "cvec_train_features = cvec.fit_transform(X_train['clean_title'])\n",
    "cvec_test_features = cvec.transform(X_test['clean_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(cvec_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is as following\n",
      "Train score:0.9401645474943904\n",
      "Test score:0.757847533632287\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the model is as following')\n",
    "print(f\"Train score:{logistic.score(cvec_train_features,y_train)}\")\n",
    "print(f\"Test score:{logistic.score(cvec_test_features, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                       analyzer = \"word\",\n",
    "                       max_features=2500,\n",
    "                       tokenizer=None,\n",
    "                       stop_words=None)\n",
    "\n",
    "tfidf_train_features = tfidf.fit_transform(X_train['clean_title'])\n",
    "tfidf_test_features = tfidf.transform(X_test['clean_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8548990276738968\n",
      "0.742152466367713\n"
     ]
    }
   ],
   "source": [
    "logistic.fit(tfidf_train_features,y_train)\n",
    "print(logistic.score(tfidf_train_features,y_train))\n",
    "print(logistic.score(tfidf_test_features,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_train_features.toarray(), columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    1302\n",
       "0.250882       1\n",
       "0.187708       1\n",
       "0.246705       1\n",
       "0.227592       1\n",
       "0.342806       1\n",
       "0.302914       1\n",
       "0.384680       1\n",
       "0.349897       1\n",
       "0.255169       1\n",
       "0.259914       1\n",
       "0.122844       1\n",
       "0.368082       1\n",
       "0.278289       1\n",
       "0.283096       1\n",
       "0.184880       1\n",
       "1.000000       1\n",
       "0.385045       1\n",
       "0.160557       1\n",
       "0.283369       1\n",
       "0.486920       1\n",
       "0.246527       1\n",
       "0.450651       1\n",
       "0.381005       1\n",
       "0.209634       1\n",
       "0.266910       1\n",
       "0.422553       1\n",
       "0.423372       1\n",
       "0.160042       1\n",
       "0.226920       1\n",
       "0.362818       1\n",
       "0.446576       1\n",
       "0.453762       1\n",
       "0.289118       1\n",
       "0.188220       1\n",
       "0.315688       1\n",
       "Name: chicken, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df['chicken'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
