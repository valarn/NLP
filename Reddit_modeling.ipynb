{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling notebook\n",
    "**Author: Vala Rahmani**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier, RandomForestClassifier\n",
    "#Import Library of Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "#Importing the Bayes and the Vader for the Sentiment Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Use this line to see all the columns\n",
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the clean csv here the csv has been formatted in the EDA notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/clean_data_vegan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the values of the clean_title that were not saved into csv\n",
    "data.dropna(subset = ['clean_title'],axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_title_selftext'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The positive class for the modeling purposes will be the subreddit Vegan however the subreddit_paleo will be used for the graphing purposes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_scores = []\n",
    "for post in data['clean_title_selftext']:\n",
    "    scores = sia.polarity_scores(post)\n",
    "    list_of_scores.append(scores)\n",
    "data['vader_score'] = list_of_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>is_video</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_vegan</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_title_selftext</th>\n",
       "      <th>vader_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a0k6zx</td>\n",
       "      <td>arav24</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_a0k6zx</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi guys! I am majoring in Nutritional Science ...</td>\n",
       "      <td>I'm a 18 year old aspiring blogger [blogspam]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm a 18 year old aspiring blogger [blogspam] ...</td>\n",
       "      <td>year old aspiring blogger</td>\n",
       "      <td>year old aspiring blogger hi guys majoring nut...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.542, 'pos': 0.458, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cb2t2d</td>\n",
       "      <td>techguySF</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_cb2t2d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did Bone Marrow make us Human? [Discussion]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Did Bone Marrow make us Human? [Discussion]</td>\n",
       "      <td>bone marrow make us human</td>\n",
       "      <td>bone marrow make us human</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id     author  is_video       name  num_comments  score  \\\n",
       "0  a0k6zx     arav24     False  t3_a0k6zx             0      0   \n",
       "1  cb2t2d  techguySF     False  t3_cb2t2d             0      0   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Hi guys! I am majoring in Nutritional Science ...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                           title  ups  subreddit_vegan  \\\n",
       "0  I'm a 18 year old aspiring blogger [blogspam]    0                0   \n",
       "1    Did Bone Marrow make us Human? [Discussion]    0                0   \n",
       "\n",
       "                                      title_selftext  \\\n",
       "0  I'm a 18 year old aspiring blogger [blogspam] ...   \n",
       "1       Did Bone Marrow make us Human? [Discussion]    \n",
       "\n",
       "                 clean_title  \\\n",
       "0  year old aspiring blogger   \n",
       "1  bone marrow make us human   \n",
       "\n",
       "                                clean_title_selftext  \\\n",
       "0  year old aspiring blogger hi guys majoring nut...   \n",
       "1                          bone marrow make us human   \n",
       "\n",
       "                                         vader_score  \n",
       "0  {'neg': 0.0, 'neu': 0.542, 'pos': 0.458, 'comp...  \n",
       "1  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a positive class for the paleo for graphing purposes\n",
    "data['subreddit_paleo'] = [1 if i==0 else 0 for i in data['subreddit_vegan']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>is_video</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_vegan</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_title_selftext</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>subreddit_paleo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>cbp9c1</td>\n",
       "      <td>stenle99</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_cbp9c1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Made some Veggie Pho the other day. A lot of e...</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>Made some Veggie Pho the other day. A lot of e...</td>\n",
       "      <td>made veggie pho day lot effort searching aroun...</td>\n",
       "      <td>made veggie pho day lot effort searching aroun...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.685, 'pos': 0.315, 'comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>cbme6u</td>\n",
       "      <td>Joey_x_G</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_cbme6u</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Found this little mad ting on r/Tinder</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>Found this little mad ting on r/Tinder</td>\n",
       "      <td>found little mad ting r tinder</td>\n",
       "      <td>found little mad ting r tinder</td>\n",
       "      <td>{'neg': 0.421, 'neu': 0.579, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    author  is_video       name  num_comments  score selftext  \\\n",
       "1781  cbp9c1  stenle99     False  t3_cbp9c1             1     45      NaN   \n",
       "1782  cbme6u  Joey_x_G     False  t3_cbme6u             1     89      NaN   \n",
       "\n",
       "                                                  title  ups  subreddit_vegan  \\\n",
       "1781  Made some Veggie Pho the other day. A lot of e...   45                1   \n",
       "1782             Found this little mad ting on r/Tinder   89                1   \n",
       "\n",
       "                                         title_selftext  \\\n",
       "1781  Made some Veggie Pho the other day. A lot of e...   \n",
       "1782            Found this little mad ting on r/Tinder    \n",
       "\n",
       "                                            clean_title  \\\n",
       "1781  made veggie pho day lot effort searching aroun...   \n",
       "1782                     found little mad ting r tinder   \n",
       "\n",
       "                                   clean_title_selftext  \\\n",
       "1781  made veggie pho day lot effort searching aroun...   \n",
       "1782                     found little mad ting r tinder   \n",
       "\n",
       "                                            vader_score  subreddit_paleo  \n",
       "1781  {'neg': 0.0, 'neu': 0.685, 'pos': 0.315, 'comp...                0  \n",
       "1782  {'neg': 0.421, 'neu': 0.579, 'pos': 0.0, 'comp...                0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************** Vegan Paleo Train Test Split *******************************#\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = [i for i in data.columns if i!='subreddit_vegan']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features],\n",
    "                                                data['subreddit_vegan'],\n",
    "                                                random_state=42,\n",
    "                                                stratify=data['subreddit_vegan'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_title_selftext</th>\n",
       "      <th>vader_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>enlightened halo top ice cream friendly</td>\n",
       "      <td>enlightened halo top ice cream friendly fine k...</td>\n",
       "      <td>{'neg': 0.118, 'neu': 0.294, 'pos': 0.588, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>belated th july burger</td>\n",
       "      <td>belated th july burger</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  clean_title  \\\n",
       "19    enlightened halo top ice cream friendly   \n",
       "1033                   belated th july burger   \n",
       "\n",
       "                                   clean_title_selftext  \\\n",
       "19    enlightened halo top ice cream friendly fine k...   \n",
       "1033                             belated th july burger   \n",
       "\n",
       "                                            vader_score  \n",
       "19    {'neg': 0.118, 'neu': 0.294, 'pos': 0.588, 'co...  \n",
       "1033  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[['clean_title','clean_title_selftext','vader_score']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train(1323, 14)\n",
      "y_train(1323,)\n",
      "X_test(442, 14)\n",
      "y_test(442,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train{X_train.shape}')\n",
    "print(f'y_train{y_train.shape}')\n",
    "print(f'X_test{X_test.shape}')\n",
    "print(f'y_test{y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.63966\n",
       "0    0.36034\n",
       "Name: subreddit_vegan, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subreddit_vegan'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model score is at 0.6397"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CountVectorizer & TfidfVectorizer then fitting into Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**First instantiate the TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the \"T\" object, which is scikit-learn's bag of words tool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    " \n",
    "\n",
    "tfid = TfidfVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                        preprocessor = None, stop_words = 'english',\n",
    "                        max_features = 1000, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "X_train_features_tfid = tfid.fit_transform(X_train['clean_title_selftext'])\n",
    "X_test_features_tfid = tfid.transform(X_test['clean_title_selftext'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate the CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the \"CountVectorizer\" object, which is scikit-learn's bag of words tool\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    " \n",
    "\n",
    "cvec = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                        preprocessor = None, stop_words = 'english',\n",
    "                        max_features = 1000, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "X_train_features_cvec = cvec.fit_transform(X_train['clean_title_selftext'])\n",
    "X_test_features_cvec = cvec.transform(X_test['clean_title_selftext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the train and the test for both methods** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1323, 1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CountVectorizer X_train and X_test\n",
    "X_train_df_cvec = pd.DataFrame(X_train_features_cvec.todense(),columns = cvec.get_feature_names())\n",
    "X_test_df_cvec = pd.DataFrame(X_test_features_cvec.todense(),columns = cvec.get_feature_names())\n",
    "\n",
    "\n",
    "# Tfid X_train and X_test\n",
    "X_train_df_tfid = pd.DataFrame(X_train_features_tfid.todense(),columns = tfid.get_feature_names())\n",
    "X_test_df_tfid = pd.DataFrame(X_test_features_tfid.todense(),columns = tfid.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different models used to determine which one is best to use in the gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DecisionTree model score for the train dataset is 0.7172284699742495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RandomForest model score for the train dataset is 0.7671827940348297\n",
      "\n",
      " AdaBoost model score for the train dataset is 0.7875746828477712\n",
      "\n",
      " KNeighbors model score for the train dataset is 0.7241036553648271\n",
      "\n",
      " LogisticRegression model score for the train dataset is 0.7921315295098811\n",
      "\n",
      " Multinomial model score for the train dataset is 0.7906020626179514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "lr = LogisticRegression()\n",
    "nbc = MultinomialNB()\n",
    "\n",
    "models = [dt, rf, ada, knn, lr,nbc]\n",
    "models_name = ['DecisionTree', 'RandomForest', 'AdaBoost', 'KNeighbors','LogisticRegression','Multinomial']\n",
    "for i in range(len(models)):    \n",
    "    print(f'\\n {models_name[i]} model score for the train dataset is {cross_val_score(models[i],X_train_df_cvec,y_train,cv=5).mean()}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating and running a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Count Vectorizer\n",
      "Train score is:0.9357520786092215\n",
      "Test score is: 0.7963800904977375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg_cvec = LogisticRegression()\n",
    "\n",
    "logreg_cvec.fit(X_train_df_cvec, y_train)\n",
    "print('Using Count Vectorizer')\n",
    "print(f'Train score is:{logreg_cvec.score(X_train_df_cvec, y_train)}')\n",
    "print(f'Test score is: {logreg_cvec.score(X_test_df_cvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countvectorizer model is very overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-IDF\n",
      "Train score is:0.8820861678004536\n",
      "Test score is: 0.7828054298642534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg_tfid = LogisticRegression()\n",
    "\n",
    "logreg_tfid.fit(X_train_df_tfid, y_train)\n",
    "print('Using TF-IDF')\n",
    "print(f'Train score is:{logreg_tfid.score(X_train_df_tfid, y_train)}')\n",
    "print(f'Test score is: {logreg_tfid.score(X_test_df_tfid, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF_IDF is also overfit but a little more acceptable than the Cvec method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class Transformer(TransformerMixin):\n",
    "    def __init__(self, *_):\n",
    "        pass\n",
    "   \n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = pd.DataFrame(X.toarray())\n",
    "        #if score is train:\n",
    "        X['neg'] = X_train.reset_index()['vader_score'].apply(lambda x: x.get('neg'))  #eval takes string and evaluates as python code\n",
    "        X['neu'] = X_train.reset_index()['vader_score'].apply(lambda x: x.get('neu'))\n",
    "        X['pos'] = X_train.reset_index()['vader_score'].apply(lambda x: x.get('pos'))\n",
    "        X['compound'] = X_train.reset_index()['vader_score'].apply(lambda x: x.get('compound'))\n",
    "        #else: use X_test vader \n",
    "        return X\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "#This code has been inspired from Erin Hwang's code and used by her permission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian using the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = Pipeline([('tfid',TfidfVectorizer()),\n",
    "               ('transformer', Transformer()),\n",
    "                ('MB', GaussianNB())])\n",
    "\n",
    "\n",
    "# Count Vectorizer is preproccessing so we have to put it in pipeline with something that can \n",
    "#scored\n",
    "pipe_params = {\n",
    "    'tfid__max_features': [1000, 2500, 3000, 3500],\n",
    "    'tfid__min_df': [2, 3],\n",
    "    'tfid__max_df': [.8, .85, .95],\n",
    "    'tfid__ngram_range': [(1,1), (1,2),(1,4)],\n",
    "    \n",
    "}\n",
    "gs_mb_tfid = GridSearchCV(pipe,param_grid=pipe_params, cv=3, n_jobs=-1,verbose=1) # 3 models for each combinations\n",
    "gs_mb_tfid.fit(X_train['clean_title_selftext'], y_train)\n",
    "print(gs_mb_tfid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch over Countvectorizer using the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   59.8s\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed:  1.1min finished\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'cvec__max_features': [2800, 3000, 3200], 'cvec__min_df': [1, 2], 'cvec__max_df': [0.2, 0.3, 0.6], 'cvec__ngram_range': [(1, 1), (1, 2)], 'lr__penalty': ['l1', 'l2'], 'lr__C': [0.01, 0.1, 10, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe = Pipeline([('cvec',CountVectorizer()),\n",
    "                 ('transform', Transformer()),\n",
    "                 ('lr', LogisticRegression())])\n",
    "\n",
    "\n",
    "\n",
    "#Count Vectorizer is preproccessing so we have to put it in pipeline with something that can \n",
    "#scored\n",
    "pipe_params = {\n",
    "     'cvec__max_features': [2800,3000,3200],\n",
    "     'cvec__min_df': [1, 2],\n",
    "     'cvec__max_df': [.2,.3,.6],\n",
    "     'cvec__ngram_range': [(1,1), (1,2)],\n",
    "     'lr__penalty': ['l1','l2'],\n",
    "     'lr__C' :[.01,.1,10,1000]\n",
    "}\n",
    "gs_cvec = GridSearchCV(pipe, param_grid=pipe_params, cv=3,verbose=1,n_jobs=-1) # 3 models for each combinations\n",
    "gs_cvec.fit(X_train['clean_title_selftext'], y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799697656840514\n",
      "{'cvec__max_df': 0.2, 'cvec__max_features': 2800, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'lr__C': 10, 'lr__penalty': 'l2'}\n",
      "score for the training data is: 0.9947089947089947\n",
      "score for the test data is: 0.7986425339366516\n",
      "f1_score for the model is: 0.8499156829679595\n",
      "[[101  58]\n",
      " [ 31 252]]\n"
     ]
    }
   ],
   "source": [
    "print(gs_cvec.best_score_)\n",
    "print(gs_cvec.best_params_)\n",
    "print(f'score for the training data is: {gs_cvec.score(X_train[\"clean_title_selftext\"], y_train)}')\n",
    "print(f'score for the test data is: {gs_cvec.score(X_test[\"clean_title_selftext\"], y_test)}')\n",
    "predictions_cvec = gs_cvec.predict(X_test['clean_title_selftext'])\n",
    "print(f'f1_score for the model is: {f1_score(predictions_cvec,y_test)}')\n",
    "print(confusion_matrix(y_test, predictions_cvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "array = confusion_matrix(y_test,predictions_cvec)\n",
    "df_cm = pd.DataFrame(array, index = [i for i in ['Actual False', 'Actual True']],\n",
    "                  columns = [i for i in ['Predicted False','Predicted True']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True,cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch over the Tfidf using the transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1728 out of 1728 | elapsed:  2.8min finished\n",
      "/Users/valamani/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799697656840514\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('tfid',TfidfVectorizer()),\n",
    "               ('transformer', Transformer()),\n",
    "                ('lr', LogisticRegression())])\n",
    "\n",
    "\n",
    "# Count Vectorizer is preproccessing so we have to put it in pipeline with something that can \n",
    "#scored\n",
    "pipe_params = {\n",
    "    'tfid__max_features': [1000, 2500, 3000, 3500],\n",
    "    'tfid__min_df': [2, 3],\n",
    "    'tfid__max_df': [.8, .85, .95],\n",
    "    'tfid__ngram_range': [(1,1), (1,2),(1,4)],\n",
    "    'lr__penalty': ['l1','l2'],\n",
    "    'lr__C' :[.01,.1,10,1000]\n",
    "}\n",
    "\n",
    "gs_tfid = GridSearchCV(pipe,param_grid=pipe_params, cv=3, n_jobs=-1,verbose=1) # 3 models for each combinations\n",
    "gs_tfid.fit(X_train['clean_title_selftext'], y_train)\n",
    "print(gs_cvec.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 10, 'lr__penalty': 'l2', 'tfid__max_df': 0.8, 'tfid__max_features': 3500, 'tfid__min_df': 2, 'tfid__ngram_range': (1, 2)}\n",
      "score for the training data is: 0.9916855631141346\n",
      "score for the test data is: 0.7895927601809954\n",
      "f1 score is 0.8436974789915966\n",
      "[[ 98  61]\n",
      " [ 32 251]]\n"
     ]
    }
   ],
   "source": [
    "print(gs_tfid.best_params_)\n",
    "print(f\"score for the training data is: {gs_tfid.score(X_train['clean_title_selftext'], y_train)}\")\n",
    "print(f\"score for the test data is: {gs_tfid.score(X_test['clean_title_selftext'], y_test)}\")\n",
    "predictions_tfid = gs_tfid.predict(X_test['clean_title_selftext'])\n",
    "print(f'f1 score is {f1_score(predictions_tfid,y_test)}')      \n",
    "print(confusion_matrix(y_test, predictions_tfid))   \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below after changing the target column we can obtain the coefficients to use in graphing in the visualization notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #IMPORTANT: In order to use change the target column to subreddit_vegan This is used for the visualization\n",
    "# feature_coefficients = dict(zip(gs_cvec.get_feature_names(), np.exp(gs_cvec.coef_[0])))\n",
    "# vegan_feature_coefficients_df = pd.DataFrame(feature_coefficients.items(),\n",
    "#                                              columns=['word', 'coefficient'])\n",
    "# vegan_feature_coefficients_df.to_csv('dataset/vegan_logistic_coef.csv',index=False,\n",
    "#                                      header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #IMPORTANT: In order to use change the target column to subreddit_paleo This is used for the visualization\n",
    "# feature_coefficients = dict(zip(cvec.get_feature_names(), np.exp(logreg_tfid.coef_[0])))\n",
    "# paleo_feature_coefficients_df = pd.DataFrame(feature_coefficients.items(), columns=['word', 'coefficient'])\n",
    "# paleo_feature_coefficients_df.to_csv('dataset/paleo_logistic_coef.csv',index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
